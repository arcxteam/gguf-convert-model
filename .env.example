# Required Configuration
HUGGINGFACE_TOKEN=hf_xxxxxxxx
REPO_ID=username/model-name

# Conversion Timer Upload
# For one-time converter, set 0
# For continuous training, converter set in seconds
CHECK_INTERVAL=7200

# Quantization types (comma-separated)
# Available: F16,BF16,Q2_K,Q2_K_S,Q3_K_S,Q3_K_M,Q3_K_L,Q4_K_S,Q4_K_M,Q4_K_L,Q5_K_S,Q5_K_M,Q5_K_L,Q6_K,Q8_0
# Recommended: F16,Q4_K_M,Q5_K_M,Q6_K
QUANT_TYPES=F16,Q3_K_M,Q4_K_M,Q5_K_M

# Upload Mode (Pick ONE)
# Option 1: same_repo - (own-repo) Upload GGUF files to the same repo as source model
# Requires: Write access to REPO_ID
UPLOAD_MODE=same_repo

# Option 2: new_repo - (create own-repo) Upload to a separate GGUF-specific repo
# Requires: Write access to TARGET_REPO (will auto-create if not exists)
# UPLOAD_MODE=new_repo
# TARGET_REPO=username/model-name-GGUF

# Option 3: local_only - Save to local folder, no HuggingFace upload
# Files saved to OUTPUT_DIR with auto-cleanup after 24 hours
# UPLOAD_MODE=local_only
# OUTPUT_DIR=./output

# Optional: Base Model for Tokenizer
# If model doesn't have complete tokenizer, specify base model
# Example for input: Qwen/Qwen3-0.6B
# Leave empty to use auto-detection or model's own tokenizer
BASE_MODEL_TOKENIZER=

# Optional: Output Filename Pattern
# Placeholders outputs â†’ Qwen3-0.6B-Q4_K_M.gguf
OUTPUT_PATTERN={model_name}-{quant}.gguf

# Optional: Local Output Cleanup
# Auto-delete local GGUF files after X hours only mode for (local_only)
LOCAL_CLEANUP_HOURS=24

# Timezone Edit Up to You
TZ=Asia/Singapore
