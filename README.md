<h1 align="center">GGUF LLMs Converter for Huggingface Hub Models with Multiple Quantizations (GGUF-Format)</h1>

<p align="center">
  <strong>Automated conversion of any Huggingface model to multiple GGUF LLMs quantization formats</strong><br>
  <em>Supports continuous monitoring, auto-detection, and universal deployment modes</em>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Release-v1.2-FF0069" alt="Version">
  <img src="https://img.shields.io/badge/GGUF-Available-brightgreen" alt="GGUF">
  <img src="https://img.shields.io/badge/llama.cpp-Compatible-orange" alt="llama.cpp">
  <img src="https://img.shields.io/badge/ğŸ¤— Huggingface-Models-blue" alt="Huggingface">
  <a href="https://github.com/arcxteam/gguf-convert-model/blob/main/LICENSE"><img src="https://img.shields.io/badge/License-MIT-green" alt="License"></a>
</p>

---

## ğŸ“– Overview

**Universal GGUF LLMs Converter** is a production-ready, Docker-based solution for automatically converting HuggingFace models to GGUF format with multiple quantization types. Built with `llama.cpp` integration and intelligent tokenizer detection, this tool streamlines the conversion workflow for both personal and community models.

### Key Features

- ğŸ”„ **Continuous Monitoring**: Automatically detects and converts new model updates from HuggingFace repositories
- ğŸ¤– **Auto-Detection**: Intelligent tokenizer detection for 50+ popular model architectures (Qwen, Llama, Mistral, Phi, Gemma, etc.)
- ğŸ“¦ **Multiple Quantization**: Supports F16, F32, BF16, and all K-quant formats (Q2_K to Q8_0)
- ğŸ¯ **Flexible Deploy**: Three (3) upload modes - same repository, new repository, or local-only storage
- ğŸ§¹ **Smart Cleanup**: Automatic temporary file management to prevent storage used
- ğŸ³ **Docker**: Fully container with optimized build times and resource usage
- ğŸ“Š **Progress Tracking**: Clean, milestone-based logging with colorized console output

## ğŸ› ï¸ Requirements

<p>
  <img src="https://img.shields.io/badge/VPS_Server-232F3E?style=for-the-badge&logo=digitalocean&logoColor=red" alt="VPS">
  <img src="https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black" alt="Linux">
  <img src="https://img.shields.io/badge/Docker-2CA5E0?style=for-the-badge&logo=docker&logoColor=white" alt="Docker">
  <img src="https://img.shields.io/badge/HuggingFace-FFD21E?style=for-the-badge&logo=huggingface&logoColor=black" alt="HuggingFace">
</p>

**System Requirements:**
- Linux-based VPS or local machine
- Docker & Docker Compose installed
- HuggingFace account with **WRITE** access token
- Sufficient disk space for model downloads and conversion (varies by model size)

## ğŸ“ Project Structure

```diff
gguf-convert-model/
â”œâ”€â”€ .env
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ start.sh
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ helpers.py
â””â”€â”€ logs/ (auto-created)
```

## ğŸš€ **Quick Start**
### 1. Prerequisites

**HuggingFace Access Token:**
- Visit settings â†’ https://huggingface.co/settings/tokens
- Create a new token with **Write** permissions
- Copy the token (starts with `hf_`)

**Install Docker & Compose** <mark>if not already installed</mark>
> Instal docker is optional, if you don't have.. try securely

```
curl -sSL https://raw.githubusercontent.com/arcxteam/succinct-prover/refs/heads/main/docker.sh | sudo bash
```

### 2. Clone Repository

```
git clone https://github.com/arcxteam/gguf-convert-model.git
cd gguf-convert-model
```

### 3. Configure Environment
> Create edit & save configuration file

```
cp .env.example .env
nano .env
```
> Example config environment variable

```diff
# HF token with WRITE permission
HUGGINGFACE_TOKEN=hf_xxxxxxxx

# Source model repository to convert
+ Example: Qwen/Qwen3-0.6B
REPO_ID=username/model-name

# Use interval in secs
+ Default 0 = only one-time convert, for other commits setup more)
CHECK_INTERVAL=0

# Output formats (comma-separated, no spaces)
# Available: F16,BF16,F32,Q2_K,Q2_K_S,Q3_K_S,Q3_K_M,Q3_K_L,Q4_K_S,Q4_K_M,Q4_K_L,Q5_K_S,Q5_K_M,Q5_K_L,Q6_K,Q8_0
+ Recommended: F16,Q4_K_M,Q5_K_M,Q6_K
QUANT_TYPES=F16,Q3_K_M,Q4_K_M,Q5_K_M,Q6_K

# ========================================
# UPLOAD MODE - Choose ONE option below
# ========================================

# OPTION 1: same_repo
# Upload to the same repository as own source model
+ Use this only YOUR OWN models with WRITE access
UPLOAD_MODE=same_repo

# OPTION 2: new_repo
# TARGET_REPO will be auto-generated as: username/ModelName-GGUF
+ Leave TARGET_REPO empty for auto (recommended)
+ Or manually specify: TARGET_REPO=your-username/custom-name-GGUF
UPLOAD_MODE=new_repo
TARGET_REPO=

# OPTION 3: local_only
+ Save to local directory only (no upload hugging)
+ Files auto-delete after LOCAL_CLEANUP_HOURS
UPLOAD_MODE=local_only
OUTPUT_DIR=./output

# Only set if auto-detection fails (default)
+ Example: Qwen/Qwen3-0.6B
BASE_MODEL_TOKENIZER=

# Output filename pattern (default)
# Placeholders: {model_name} = extracted base name, {quant} = format type
+ Result example: Qwen3-0.6B-Instruct-Q4_K_M.gguf
OUTPUT_PATTERN={model_name}-{quant}.gguf

# Auto-cleanup hours (default)
+ Setup you need local_only mode
LOCAL_CLEANUP_HOURS=24

# Timezone
TZ=Asia/Singapore
```

## ğŸ“Š **Configuration Reference**

| ENV Variable | Required? | When to Change | Default if Empty |
|--------------|-----------|----------------|------------------|
| `HUGGINGFACE_TOKEN` | âœ… Yes | Always (your token) | `ERROR` |
| `REPO_ID` | âœ… Yes | Always (source model) | `ERROR` |
| `CHECK_INTERVAL` | âš ï¸ Optional | Default= 0 or Changes | `in secs 3600=1h` |
| `QUANT_TYPES` | âš ï¸ Optional | Change formats needed | `F16,Q4_K_M,Q5_K_M,more` |
| `UPLOAD_MODE` | âš ï¸ Optional | Change based on use case | default `new_repo` |
| `TARGET_REPO` | âš ï¸ Conditional | Only if `new_repo` mode | Same as `REPO_ID` |
| `OUTPUT_DIR` | âš ï¸ Conditional | Only if `local_only` mode | `./output` |
| `BASE_MODEL_TOKENIZER` | âŒ Optional | Only if auto-detect fails | `empty = auto` |
| `OUTPUT_PATTERN` | âŒ Optional | Only if custom naming | `{model_name}-{quant}.gguf` |
| `LOCAL_CLEANUP_HOURS` | âŒ Optional | Only for `local_only` | default `24hour` |
| `TZ` | âŒ Optional | Change to your timezone | UTC |

### âœ… Checklist - What to Change

**Always Change:**
- âœ… `HUGGINGFACE_TOKEN` â†’ Your personal token
- âœ… `REPO_ID` â†’ Model to convert

**Usually Change:**
- âš ï¸ `CHECK_INTERVAL` â†’ Frequency (or 0 for one-time)
- âš ï¸ `QUANT_TYPES` â†’ Formats you need
- âš ï¸ `UPLOAD_MODE` â†’ Based on use case

**Change Only If Needed:**
- âŒ `TARGET_REPO` â†’ If using `new_repo` mode
- âŒ `OUTPUT_DIR` â†’ If using `local_only` mode
- âŒ `BASE_MODEL_TOKENIZER` â†’ If auto-detect fails
- âŒ `OUTPUT_PATTERN` â†’ If custom naming wanted
- âŒ `LOCAL_CLEANUP_HOURS` â†’ If different cleanup time
- âŒ `TZ` â†’ Your timezone (up to you)

**Never Change (Leave Default):**
- âœ… Comments (helpful documentation)
- âœ… Commented-out options (for reference)

### 3.ğŸƒ **Build and Start**
> Starting running

```
docker compose up --build -d
```

> Monitor logs & stop

```
docker compose logs -f
# docker compose down
```

## ğŸ“Š **Supported Quantization Formats**

| Format | Precision | Size Reduction | Use Case |
|--------|-----------|----------------|----------|
| **F32** | Full (32-bit) | None | Maximum precision |
| **F16** | Half (16-bit) | ~50% | High quality general use |
| **BF16** | Brain Float 16 | ~50% | Training-optimized |
| **Q8_0** | 8-bit | ~75% | Near-lossless compression |
| **Q6_K** | 6-bit | ~80% | High quality compression |
| **Q5_K_M** | 5-bit | ~83% | **Recommended** balance |
| **Q4_K_M** | 4-bit | ~87% | **Popular** for production |
| **Q3_K_M** | 3-bit | ~90% | Aggressive compression |
| **Q2_K** | 2-bit | ~93% | Maximum compression |

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
